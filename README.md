# prompt_tunign
## Business Objective:
This project aimed to streamline the process of creating effective prompts for Large Language Models (LLMs) like GPT-3. By automating prompt engineering, businesses can:
- Generate high-quality prompts efficiently.
- Reduce time and expertise required for manual prompt crafting.
  
---

### Tech Stack:
- Front-End: Streamlit
- Vector Database: ChromaDB
- Prompt Engineering: Langchain Library
- Large Language Model: OpenAI GPT-3
- Evaluation: Ragas
  
### Key Accomplishments:
- **Data Preprocessing:** Built a function to organize and clean text data from the Fabric repository.
- **Scalable Chunking:** Chunked prompts into manageable sizes for efficient LLM processing.
- **Tokenization:** Implemented tokenization to estimate embedding costs and understand data size.
- **ChromaDB Integration:** Leveraged ChromaDB to store prompts in a vector database for efficient retrieval.
- **Langchain Integration:** Employed Langchain within the RAG system to generate effective retrieval prompts. This allows the system to find relevant information within the embedded data for improved LLM responses.
